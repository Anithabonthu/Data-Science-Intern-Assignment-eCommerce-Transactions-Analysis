# -*- coding: utf-8 -*-
"""DataScienceProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T3yDaTtYuZy_Ks_bdNOtEvw0N2pFLhOP
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the datasets
customers = pd.read_csv("/content/Customers.csv")
products = pd.read_csv("/content/Products.csv")
transactions = pd.read_csv("/content/Transactions.csv")

# Check the first few rows
print(customers.head(), products.head(), transactions.head())

# Check for missing values
print(customers.isnull().sum())
print(products.isnull().sum())
print(transactions.isnull().sum())

# Check data types
print(customers.info(), products.info(), transactions.info())

# Summary statistics
print(transactions.describe())

# Count of customers by region
plt.figure(figsize=(8, 5))
sns.countplot(y=customers['Region'], order=customers['Region'].value_counts().index)
plt.title("Customer Count by Region")
plt.show()

# Top 10 most sold products
top_products = transactions.groupby('ProductID')['Quantity'].sum().nlargest(10)
top_products.plot(kind='bar', title="Top 10 Most Sold Products")
plt.show()

# Trend of transactions over time
transactions['TransactionDate'] = pd.to_datetime(transactions['TransactionDate'])
transactions.set_index('TransactionDate').resample('M').size().plot(title="Monthly Transaction Volume")
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Merge transactions with customers and products
df = transactions.merge(customers, on="CustomerID").merge(products, on="ProductID")

# Create a "profile" for each customer based on purchased products
customer_profiles = df.groupby("CustomerID")["ProductName"].apply(lambda x: ' '.join(x))

# Convert text data into numerical form using TF-IDF
vectorizer = TfidfVectorizer()
customer_vectors = vectorizer.fit_transform(customer_profiles)

# Compute similarity matrix
similarity_matrix = cosine_similarity(customer_vectors)

# Create lookalike recommendations
customer_ids = list(customer_profiles.index)
lookalikes = {}

for i, customer in enumerate(customer_ids[:20]):  # First 20 customers
    similar_customers = sorted(
        list(enumerate(similarity_matrix[i])), key=lambda x: x[1], reverse=True
    )[1:4]  # Top 3 excluding self

    lookalikes[customer] = [(customer_ids[idx], round(score, 2)) for idx, score in similar_customers]

# Convert to DataFrame and Save
lookalike_df = pd.DataFrame(lookalikes.items(), columns=["CustomerID", "Lookalikes"])
lookalike_df.to_csv("Lookalike.csv", index=False)
print(lookalike_df.head())

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import davies_bouldin_score

# Create customer purchase summary
customer_summary = transactions.groupby("CustomerID").agg(
    {"TotalValue": "sum", "TransactionID": "count"}
).rename(columns={"TotalValue": "TotalSpend", "TransactionID": "PurchaseCount"})

# Standardize the data
scaler = StandardScaler()
customer_data_scaled = scaler.fit_transform(customer_summary)

# Find optimal clusters (DB Index)
db_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(customer_data_scaled)
    db_index = davies_bouldin_score(customer_data_scaled, labels)
    db_scores.append(db_index)

# Choose the number of clusters with the lowest DB Index
optimal_k = range(2, 11)[db_scores.index(min(db_scores))]
print(f"Optimal clusters: {optimal_k}")

# Fit the best KMeans model
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
customer_summary["Cluster"] = kmeans.fit_predict(customer_data_scaled)

# Visualize clusters
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=customer_summary["TotalSpend"], y=customer_summary["PurchaseCount"],
    hue=customer_summary["Cluster"], palette="viridis", alpha=0.7
)
plt.title("Customer Segmentation Clusters")
plt.xlabel("Total Spend")
plt.ylabel("Purchase Count")
plt.show()